## Sources of Data
1. Web Application:
    - Database
    - Logs and Event data: Server logs, application logs, clickstream data
    - APIs and Web Services: Data retrieved from various APIs (e.g., RESTful APIs, GraphQL) and web services.

2. Mobile Apps: Data collected from mobile applications, such as user interactions, usage patterns, and app-generated events.

3. IoT Devices: Data generated by Internet of Things (IoT) devices, sensors, and machines.

4. Web Scraping: Data extracted from websites through web scraping techniques.
    - Social Media: Data collected from social media platforms, including posts, comments, likes, and other interactions.

5. Legacy Systems: Data from older, existing systems that need to be integrated into modern data architectures.

6. External Data Sources: Data from external partners, third-party providers, or data syndication feeds.
    - Economic Indicators: Incorporating economic indicators such as GDP, unemployment rates, inflation data, and consumer spending from government agencies or economic research institutions.

    - Environmental Data: Accessing environmental data, such as pollution levels, air quality indices, and climate data, from relevant sources

    - Social and Cultural Data: Collecting data on cultural trends, social behaviors, and societal shifts from relevant sources for sociological or cultural analysis.

    - News Feeds: Integrating news articles, headlines, and updates from news agencies or RSS feeds to stay informed about current events relevant to your industry.

    - Geographic Data Services: Using geographic data sources like Google Maps APIs or OpenStreetMap data for mapping, geolocation, or spatial analysis.


Direct Dump / Raw Data -> DataLake:

## DataLake

- Data Cleaning
- Data Transformation
- Data Validation
    How do you validate a data set and know it's high quality? Learning how to check for things like duplicates, weird distributions, NULLs, etc. SQL skills will help you greatly here!
    1. Source data availability:
    Data pipelines get populated by data from various source systems. Include checks to make sure we are consuming the latest data. Ex (current_date -1) data should be available in the source.

    2. Data drop percentage:
    If we are consuming data from tables which run on incremental loads then daily we should see a trend of increase in data records. Include checks to ensure there is no huge data drop for a given job run.

    3. Duplicate count:
    Data having primary keys should always be populated with unique values. Include checks to filter out duplicates and report the data issue to the source teams.

    4. Data type mismatch:
    Include checks to make sure the data type fields being consumed and populated in our tables are aligning with each other else they can raise issues during processing.

    5. Valid codes match:
    Columns having standard values for timezone, currency type etc should be populated with  only a list of allowed values and checks can be included for the same.
## Data Warehouse:


Data Modelling Techniques
- Dimemsions and Facts
- Data Vault
- One Big Table
- Aggregations
- Indexing and partitioning

##
Finally to =>  AI / ML / reports
